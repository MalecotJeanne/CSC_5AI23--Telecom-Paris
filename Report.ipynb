{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VQVAE for Image Generation - FashionMNIST Dataset\n",
    "**Author:** Jeanne Malécot "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Started"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#useful imports\n",
    "import os\n",
    "import copy\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torchinfo\n",
    "import random\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from scripts.train import train_model\n",
    "from scripts.reconstruct import reconstruct, show_recon\n",
    "from models.vqvae import VQVAE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Load FashionMNIST dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len train set: 60000\n",
      "len test set: 10000\n",
      "image shape: torch.Size([28, 28])\n"
     ]
    }
   ],
   "source": [
    "#load FashionMNIST\n",
    "train_set = torchvision.datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=transforms.ToTensor()\n",
    ")\n",
    "test_set = torchvision.datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=transforms.ToTensor()\n",
    ")\n",
    "\n",
    "print(f\"len train set: {len(train_set)}\\nlen test set: {len(test_set)}\")\n",
    "print(f\"image shape: {train_set[0][0][0].shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training VQVAE for reconstruction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cuda\n"
     ]
    }
   ],
   "source": [
    "#device\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print('device:', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#config\n",
    "config = {\n",
    "    'n_epochs': 10, \n",
    "    'lr': 0.001, \n",
    "    'alpha': 0.25,\n",
    "    'model': {\n",
    "        'batch_size': 100, \n",
    "        'n_channels': 1, \n",
    "        'channels': [32, 64, 128],\n",
    "        'latent_dim': 32,\n",
    "        'n_embedding': 128,\n",
    "        'beta': 1.2,\n",
    "        }\n",
    "    } "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model architecture:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "===============================================================================================\n",
       "Layer (type:depth-idx)                        Output Shape              Param #\n",
       "===============================================================================================\n",
       "VQVAE                                         [1, 1, 28, 28]            --\n",
       "├─EncoderBlock: 1-1                           [1, 32, 7, 7]             --\n",
       "│    └─ModuleList: 2-1                        --                        --\n",
       "│    │    └─Sequential: 3-1                   [1, 32, 28, 28]           320\n",
       "│    │    └─Sequential: 3-2                   [1, 64, 14, 14]           18,496\n",
       "│    │    └─Sequential: 3-3                   [1, 128, 7, 7]            73,856\n",
       "│    │    └─Sequential: 3-4                   [1, 32, 7, 7]             4,128\n",
       "├─VectorQuantizer: 1-2                        [1, 32, 7, 7]             4,096\n",
       "├─DecoderBlock: 1-3                           [1, 1, 56, 56]            --\n",
       "│    └─ModuleList: 2-2                        --                        --\n",
       "│    │    └─Sequential: 3-5                   [1, 128, 7, 7]            4,224\n",
       "│    │    └─Sequential: 3-6                   [1, 64, 14, 14]           32,832\n",
       "│    │    └─Sequential: 3-7                   [1, 32, 28, 28]           8,224\n",
       "│    │    └─Sequential: 3-8                   [1, 1, 56, 56]            129\n",
       "├─Conv2d: 1-4                                 [1, 1, 56, 56]            2\n",
       "===============================================================================================\n",
       "Total params: 146,307\n",
       "Trainable params: 146,307\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (Units.MEGABYTES): 21.20\n",
       "===============================================================================================\n",
       "Input size (MB): 0.00\n",
       "Forward/backward pass size (MB): 0.77\n",
       "Params size (MB): 0.57\n",
       "Estimated Total Size (MB): 1.34\n",
       "==============================================================================================="
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vqvae = vqvae = VQVAE(config['model']).to(device)\n",
    "torchinfo.summary(vqvae, (1, 1, 28,28), device = str(device))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fine tuning of the VQ-VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80451fa73f8a411b845152f53641b6b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# config = copy.deepcopy(basic_config)\n",
    "model_dict, loss_dict = train_model(\"vqvae\", train_set, config, device)\n",
    "print(f\"Loss: {model_dict['train_loss']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_map = {\n",
    "    0: \"T-Shirt\",\n",
    "    1: \"Trouser\",\n",
    "    2: \"Pullover\",\n",
    "    3: \"Dress  \",\n",
    "    4: \"Coat\",\n",
    "    5: \"Sandal\",\n",
    "    6: \"Shirt\",\n",
    "    7: \"Sneaker\",\n",
    "    8: \"Bag\",\n",
    "    9: \"Ankle Boot\",\n",
    "}\n",
    "model = model_dict['model']\n",
    "\n",
    "image_dicts, label_dist = reconstruct(model, test_set, device)\n",
    "show_recon(image_dicts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = loss_dict['loss']\n",
    "vq_loss = loss_dict['vq_loss']\n",
    "reconstruction_loss = loss_dict['reconstruction_loss']\n",
    "\n",
    "x = np.arange(len(loss))\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "plt.plot(x, loss, label='final loss')\n",
    "plt.plot(x, vq_loss, label='vq_loss')\n",
    "plt.plot(x, reconstruction_loss, label='reconstruction loss')\n",
    "\n",
    "plt.legend()\n",
    "plt.ylim(ymin=0)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert vector into p_dist\n",
    "def to_prob(vector, temperature=1):\n",
    "    if temperature <= 0:\n",
    "        raise ValueError(\"Carreful ! Temperature must be greater than 0.\")\n",
    "    \n",
    "    scaled_vector = vector / temperature\n",
    "    prob_dist = torch.softmax(scaled_vector, dim=0)\n",
    "    return prob_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate sneakers\n",
    "dist = (label_dist[7])\n",
    "\n",
    "sneakers_dist= to_prob(dist, torch.std(dist)**2)\n",
    "# print(sneakers_dist)\n",
    "\n",
    "generated_image = model.generate(sneakers_dist, (28,28))\n",
    "print(generated_image.shape)\n",
    "plt.imshow(generated_image.detach().cpu().numpy(), cmap = 'gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3 (jupytervenv)",
   "language": "python",
   "name": "jupyterenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
