{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VQVAE for Image Generation - FashionMNIST Dataset\n",
    "**Author:** Jeanne Malécot "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the report notebook for the Optional Homework n°1 (IA323).  \n",
    "Most of the functions used are defined in Python scripts, to make the notebook lighter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Started"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#useful imports\n",
    "import os\n",
    "import copy\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torchinfo\n",
    "import random\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from scripts.train import train_model\n",
    "from scripts.reconstruct import reconstruct, show_recon, plot_latent_space\n",
    "from models.vqvae import VQVAE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Load FashionMNIST dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5], std=[0.5]) \n",
    "])\n",
    "\n",
    "#load FashionMNIST\n",
    "train_set = torchvision.datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=transform\n",
    ")\n",
    "test_set = torchvision.datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "print(f\"len train set: {len(train_set)}\\nlen test set: {len(test_set)}\")\n",
    "print(f\"image shape: {train_set[0][0][0].shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Fashion MNIST dataset consists of 70,000 images, divided into a training set and a test set.   \n",
    "Each image measures 28x28, has a single channel (greyscale) and is associated with a label (from a list of 10)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_map = {\n",
    "    0: \"T-Shirt\",\n",
    "    1: \"Trouser\",\n",
    "    2: \"Pullover\",\n",
    "    3: \"Dress  \",\n",
    "    4: \"Coat\",\n",
    "    5: \"Sandal\",\n",
    "    6: \"Shirt\",\n",
    "    7: \"Sneaker\",\n",
    "    8: \"Bag\",\n",
    "    9: \"Ankle Boot\",\n",
    "}\n",
    "\n",
    "#visualisation\n",
    "figure = plt.figure(figsize=(8, 8))\n",
    "cols, rows = 3, 3\n",
    "\n",
    "for i in range(1, cols * rows + 1):\n",
    "    sample_idx = torch.randint(len(train_set), size=(1,)).item()\n",
    "    img, label = train_set[sample_idx]\n",
    "    figure.add_subplot(rows, cols, i)\n",
    "    plt.title(labels_map[label])\n",
    "    plt.axis(\"off\")\n",
    "    plt.imshow(img.squeeze().numpy(), cmap=\"gray\") \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to the dimensions, the image are not really precise, so we will focus on the generation of a identifiable shape, and not the obtention of identical textures in reconstruction for example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training VQVAE for reconstruction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#device\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print('device:', device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model's arguments are provided by a config dict so that the fine tuning will be easier from this notebook.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to the 'usual' parameters of a model training, we will deal with:\n",
    "\n",
    "- `gamma`: balances the VQ loss and the reconstruction loss*, in the final loss (used for back-porpagation)\n",
    "  \n",
    "And parameters specifics to the VQ VAE architecture:\n",
    "  \n",
    "- `latent_dim`: the embedding dimension\n",
    "- `n_embeddings`: the number of embeddings\n",
    "- `beta`: the commitement cost (for the commitement loss of the VQ)\n",
    "\n",
    "\n",
    "_* about the reconstruction los, I used the MSE loss, that seems the more pertienent to get accurate reconstruction. Maybe it could have been usefull to add a loss that would learn more 'global' informations about the shape, for example (I am thinking of a KLD loss, for example)._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#config\n",
    "config = {\n",
    "    'n_epochs': 15, \n",
    "    'lr': 0.001, \n",
    "    'gamma': 0.7,\n",
    "    'model': {\n",
    "        'batch_size': 200, \n",
    "        'n_channels': 1, \n",
    "        'channels': [128, 256, 512],\n",
    "        'latent_dim': 64,\n",
    "        'n_embedding': 512,\n",
    "        'beta': 0.5,\n",
    "        }\n",
    "    } "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The VQ-VAE has encoder-decoder architecture, and I went for 4 downsampling (and upsampling) steps, that required to augment the size of the images (they have to be divisible by 2^4, to avoid shapes mismatching with up-convolutions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vqvae = vqvae = VQVAE(config['model']).to(device)\n",
    "torchinfo.summary(vqvae, (1, 1, 28,28), device = str(device))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fine tuning of the VQ-VAE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training loop is also defined in an additional script, that I ran for many different configurations.  \n",
    "I also implemented a grid-search executable script, to help me fine-tuning the parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dict, loss_dict = train_model(\"vqvae\", train_set, config, device)\n",
    "print(f\"Loss: {model_dict['train_loss']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = loss_dict['loss']\n",
    "vq_loss = loss_dict['vq_loss']\n",
    "reconstruction_loss = loss_dict['reconstruction_loss']\n",
    "\n",
    "x = np.arange(len(loss))\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "plt.plot(x, loss, label='final loss')\n",
    "plt.plot(x, vq_loss, label='vq_loss')\n",
    "plt.plot(x, reconstruction_loss, label='reconstruction loss')\n",
    "\n",
    "plt.legend()\n",
    "plt.ylim(ymin=0, ymax=2)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model_dict['model']\n",
    "\n",
    "image_dicts, label_dist, reduced_indices, latent_labels = reconstruct(model, test_set, device)\n",
    "show_recon(image_dicts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The latent indices (representation in the embedding space) of an image are saved during the forward pass, so while reconstructing the test_set, we are able to learn a distribution of the different embeddings for **each label**, that should make us able to generate an image of one particular label, by sampling the vectors to decode according to this distribution.  \n",
    "\n",
    "We can use a PCA with 2 components on the latent indices of each reconstructed images to visualize the embedding space in 2 dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_latent_space(reduced_indices, latent_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is hard to distinguish the 10 different labels on the plot, which means that the model has pain to learn really specific representations for each labels.\n",
    "However, the way that some labels are closers to others makes sense: It seems normal that the representation of a sneaker will be close to the representation of a sandal for example: the global shape is quiet the same."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To sample new vectors, we have to convert the the vectors obtained into probability distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_prob(vector):\n",
    "    label_distribution = vector /( vector.sum() + 1e-3 )\n",
    "    return label_distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I could have used a Softmax to obtain a probability distribution, but the presence of both zero and really high values required to add a temperature, that I had a hard-time to determine (and was seemed to be dependant on the label chosen) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 5, figsize=(15, 6))\n",
    "\n",
    "for label in range(10):\n",
    "    dist = label_dist[label] \n",
    "    label_prob = to_prob(dist)  \n",
    "    generated_image = model.generate(label_prob, (28, 28))  \n",
    "\n",
    "    row = label // 5\n",
    "    col = label % 5\n",
    "\n",
    "    axes[row, col].imshow(generated_image.detach().cpu().numpy(), cmap='gray')\n",
    "    axes[row, col].set_title(f\"{labels_map[label]}\")\n",
    "    axes[row, col].axis('off')  \n",
    "    \n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The generations of new images from those distributions are not really convincing... \n",
    "We can see different patterns for each label, but none of them looks like an image. The way I sample the vectors is probably in cause, but I was not able to fix it in the time I had despite a few attempts.\n",
    "\n",
    "The training time and access to resources were also an issue: I would perhaps have had better results by just increasing the depth of the model or training it on more epochs, but I am not that sure that it was the only issue..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3 (jupytervenv)",
   "language": "python",
   "name": "jupyterenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
